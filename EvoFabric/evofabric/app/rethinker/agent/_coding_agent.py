# -*- coding: utf-8 -*-
# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
import re
import time
from typing import Annotated, Awaitable, Callable, Optional

from jinja2 import Template
from pydantic import BaseModel, Field, PrivateAttr
from rich.console import Console, Group
from rich.panel import Panel
from rich.text import Text

from evofabric.core.clients import ChatClientBase
from evofabric.core.factory import FactoryTypeAdapter
from evofabric.core.graph import current_ctx, StreamCtx
from evofabric.core.typing import LLMChatResponse
from evofabric.logger import get_logger

_DEFAULT_CHAT_TEMPLATE = """{%- for message in tool_logs -%}
{%- if message['role'] == 'system' -%}
{{ message['content'] }}
{%- endif -%}
{%- if message['role'] == 'user' -%}
<｜User｜> {{ message['content'] }}
{%- endif -%}
{%- if message['role'] == 'assistant' -%}
<｜Assistant｜>
{%- if message.get('reasoning_content') -%}
<think>{{ message['reasoning_content'] }}</think>
{%- endif -%}
{{ message['content'] }}
{%- endif -%}
{%- if message['role'] == 'tool_call' -%}
<code>{{ message['content'] }}</code>
  {%- elif message['role'] == 'tool_call_result' -%}
<execution_results>{{ message['content'] }}</execution_results>
  {%- endif -%}
{%- endfor -%}
"""


def _extract_tool_content(input_str: str, pattern: str):
    """
    Extract structured tool content from a model-generated string using a regex pattern.

    Args:
        input_str (str):
            The full text generated by the model, potentially containing
            natural language reasoning and embedded tool/code blocks.
        pattern (str):
            A regular expression pattern with at least one capturing group.
            The first capturing group is expected to contain the tool content
            (e.g., Python code) to be extracted.

    Returns:
        Tuple[str, str]:
            A two-element tuple:
            - The prefix text before the last matched tool block. This typically
              represents the model's reasoning or explanatory content.
            - The extracted tool content (e.g., code) from the last match.
              If no match is found, this value is an empty string.
    """
    matches = list(re.finditer(pattern, input_str, re.DOTALL))
    detected_num = len(matches)

    if detected_num > 0:
        match = matches[-1]
        code_content = match.group(1)
        match_start_index = match.start()
        cut_text = input_str[:match_start_index]

        return cut_text, code_content

    return input_str, ''


def _has_signal(input_str: str, pattern_condition: str) -> bool:
    """
    Check whether a specific structural signal exists in the input string.

    Args:
        input_str (str):
            The text to be inspected, usually a model-generated response.
        pattern_condition (str):
            A regular expression pattern representing the signal to detect
            (e.g., a final answer tag).

    Returns:
        bool:
            True if at least one match of the pattern is found in the input string;
            False otherwise.
    """
    matches = list(re.finditer(pattern_condition, input_str, re.DOTALL))
    detected_num = len(matches)
    if detected_num > 0:
        return True
    else:
        return False


class CodingAgentResult(BaseModel):
    ctx: StreamCtx
    """Execution context captured from the current graph node."""

    total_steps: int
    """Total number of reasoning / action steps performed by the agent
    before termination."""

    generation_time: float
    """End-to-end wall-clock time (in seconds) spent on the agent execution,
    including LLM calls and tool invocations."""

    agent_logs: list
    """Complete conversational trace maintained by the agent."""

    client_responses: list
    """Raw responses returned by the LLM client at each iteration."""

    response: Optional[str] = None
    """Final rendered response generated from the agent logs using the
    configured chat template.
    If no chat template is provided, this field is set to None.
    """


class CodingAgent(BaseModel):
    """
    An autonomous LLM-driven coding agent that performs multi-step iterative reasoning
    with native Python execution support.

    This agent maintains a full conversational trajectory and repeatedly:
    1) queries an LLM backend,
    2) parses structured signals from the model output (e.g., code blocks or final answers),
    3) executes generated Python code via a user-provided executor,
    4) feeds execution results back into the dialogue,
    until a termination condition is met.

    Core capabilities:
    - Iterative reasoning with a configurable maximum number of steps
    - Structured tool (Python code) extraction and execution
    - Automatic fallback prompting when the model produces empty or invalid responses
    - Pattern-based detection of final answers
    - Detailed step-level logging and execution tracing

    The agent is designed to be used as an asynchronous callable node within a larger
    execution graph or workflow system.

    Execution behavior:
    - The agent iteratively queries the LLM until either:
        * a final answer matching `answer_pattern` is detected, or
        * the maximum number of steps (`max_agent_step`) is reached, or
        * the maximum number of consecutive empty responses is exceeded.
    - If Python code matching `tool_content_pattern` is detected,
      it is executed via `py_exec_handler(code, timeout)`, and the
      execution result is injected back into the dialogue.
    - When the model repeatedly produces empty or invalid outputs,
      fallback prompts from `force_finish_prompt_candidates` are used
      to encourage termination.
    """

    client: Annotated[ChatClientBase, FactoryTypeAdapter, Field()]
    """LLM chat client used to issue generation requests during each agent step."""

    max_agent_step: int = 30
    """Maximum agent reasoning / action steps"""

    max_empty_response: int = 5
    """Maximum number of consecutive empty agent responses 
    (i.e., responses that match neither code nor answer patterns)"""

    tool_timeout: int = 300
    """Timeout for tool execution"""

    force_finish_prompt_candidates: list = Field(default_factory=list)
    """Fallback prompt candidates applied when the agent returns an empty response,
    prompting the agent to produce a final answer and end the dialogue."""

    tool_content_pattern: str = Field(default='<code[^>]*>((?:(?!<code).)*?)</code>')
    """Regex pattern used to extract tool (code) content from the agent response"""

    answer_pattern: str = Field(default='<answer[^>]*>((?:(?!<answer).)*?)</answer>')
    """Regex pattern used to extract the final answer content from the agent response"""

    chat_template: str = Field(default=None)
    """Chat prompt template used to format agent interation history into a single string.
    If chat template is none, no response field will be output."""

    py_exec_handler: Callable[[str, int], Awaitable[BaseModel]] = Field(default=None)
    """Handler function for executing generated Python code,
    typically taking the code string and a timeout value as input"""

    _console: Console = PrivateAttr(default_factory=Console)
    _agent_logs: list = PrivateAttr(default_factory=list)

    def _get_log_context(self, graph_ctx) -> dict:
        """
        Extract logging-related context information from the graph context.

        This method is intended to be overridden by subclasses to customize
        which metadata fields are displayed in agent logs.

        Args:
            graph_ctx:
                The current graph execution context, typically containing
                node information and optional metadata.

        Returns:
            dict:
                A dictionary of key-value pairs representing contextual
                information to be displayed in logs (e.g., query_id).
                An empty dict indicates no contextual information.
        """
        if not graph_ctx.meta:
            return {}

        return graph_ctx.meta

    def _log_agent_step(
        self,
        start_time: float,
        step: int,
        reasoning: str,
        content: str,
        log_ctx: dict,
        node_name: str,
        tool_res: dict = None,
        end_reason: str = None
    ):
        """Logs a single step of the agent's execution process to the console.

        Constructs a Rich Panel containing the query index, reasoning, main content,
        tool results (if any), and execution status.

        Args:
            start_time: The timestamp when the process started.
            step: The current step number in the iteration.
            reasoning: The chain-of-thought or reasoning text from the model.
            content: The main response content from the model.
            log_ctx: The context information to log.
            node_name: The name of the current graph node (used as Panel title).
            tool_res: A dictionary containing results from a tool call, if applicable.
            end_reason: A string describing why the process finished, if applicable.
        """
        reasoning = reasoning.strip()
        content = content.strip()

        renderables = []

        log_ctx = log_ctx or {}
        if log_ctx:
            for k, v in log_ctx.items():
                renderables.append(
                    Text(f"{k}: {v}", style="bold magenta")
                )
            renderables.append(Text("─" * 40, style="dim"))

        if reasoning:
            renderables.append(Text("Reasoning:", style="bold yellow"))
            renderables.append(Text(reasoning, style="dim italic"))
            renderables.append(Text("─" * 40, style="dim"))

        renderables.append(Text("Content:", style="bold green"))
        renderables.append(Text(content or ""))

        if tool_res:
            renderables.append(Text("─" * 40, style="dim"))
            renderables.append(Text("Tool Result:", style="bold blue"))
            renderables.append(Text(str(tool_res), style="cyan", overflow="fold"))

        border_style = "white"
        subtitle_info = f"Step: {step} | Time: {time.time() - start_time:.2f}s"

        if end_reason:
            subtitle_info += f" | FINISHED: {end_reason}"
            border_style = "green"
        elif tool_res:
            subtitle_info += " | TOOL CALL"
            border_style = "blue"

        self._console.print(Panel(
            Group(*renderables),
            title=f"[bold]{node_name}[/bold]",
            subtitle=subtitle_info,
            border_style=border_style,
            expand=True
        ))

    async def __call__(self, prompt: str) -> CodingAgentResult:
        """
        Run the coding agent on a single input prompt.

        Args:
            prompt (str):
                The initial user prompt provided to the agent.
                This prompt is appended as the first user message in the internal
                conversation history and serves as the starting point for all
                subsequent reasoning and tool interactions.

        Returns:
            CodingAgentResult:
                A structured result object containing:
                - ctx:
                    Serialized execution context from the current graph node
                    (e.g., node name, metadata such as query_id).
                - total_steps (int):
                    The total number of agent steps executed before termination.
                - generation_time (float):
                    End-to-end wall-clock time (in seconds) for the entire agent run.
                - agent_logs (list):
                    The full conversation history, including user prompts,
                    assistant responses, reasoning content, and tool execution feedback.
                - client_responses (list):
                    Raw responses returned by the LLM backend at each step
                    (stored as serialized dictionaries).
                - response (Optional[str]):
                    A rendered string produced by applying `chat_template`
                    to the accumulated agent logs. If no chat template is provided,
                    this field is set to None.
        """
        if self.py_exec_handler is None:
            raise ValueError("CodingAgent must set a function defined as py_exec_handler(code, timeout) -> BaseModel")

        if not self.force_finish_prompt_candidates:
            self.force_finish_prompt_candidates = [
                f"Now combine all the information above and write the answer matches this pattern: {self.answer_pattern}"
            ]

        logger = get_logger()
        graph_ctx = current_ctx()
        node_name = graph_ctx.node_name
        log_ctx = self._get_log_context(graph_ctx)

        self._agent_logs.append({"role": "user", "content": prompt})
        empty_response_count = 0
        cur_steps = 0
        client_responses = []
        e2e_start_time = time.time()

        while cur_steps <= self.max_agent_step:
            start_time = time.time()
            result: LLMChatResponse = await self.client.create(self._agent_logs)
            client_responses.append(result.model_dump())

            step_response, tool_call_content = _extract_tool_content(result.content, self.tool_content_pattern)
            self._agent_logs.append(
                {"role": "assistant", "content": result.content, "reasoning_content": result.reasoning_content})
            end_reason = "Meet finish condition" if _has_signal(result.content, self.answer_pattern) else None

            if end_reason:
                self._log_agent_step(
                    start_time=start_time,
                    step=cur_steps,
                    reasoning=result.reasoning_content,
                    content=result.content,
                    log_ctx=log_ctx,
                    node_name=node_name,
                    end_reason=end_reason
                )
                break

            if tool_call_content:
                tool_result = await self.py_exec_handler(tool_call_content, self.tool_timeout)
                self._agent_logs.append(
                    {
                        "role": "user",
                        "content": f"<execution_results>{tool_result.model_dump()}</execution_results>"
                    }
                )

                self._log_agent_step(
                    start_time=start_time,
                    step=cur_steps,
                    reasoning=result.reasoning_content,
                    content=result.content,
                    log_ctx=log_ctx,
                    node_name=node_name,
                    tool_res=tool_result.model_dump()
                )
            else:
                self._log_agent_step(
                    start_time=start_time,
                    step=cur_steps,
                    reasoning=result.reasoning_content,
                    content=result.content,
                    log_ctx=log_ctx,
                    node_name=node_name
                )

                empty_response_count += 1
                if empty_response_count > self.max_empty_response:
                    logger.warning(
                        f"Max empty responses reached (do not match patterns: {self.answer_pattern} and "
                        f"{self.tool_content_pattern}), stopping.")
                    break

                self._agent_logs.append({
                    "role": "user",
                    "content": self.force_finish_prompt_candidates[
                        empty_response_count % len(self.force_finish_prompt_candidates)]
                })

            cur_steps += 1
        return CodingAgentResult(
            ctx=graph_ctx.model_dump(),
            total_steps=cur_steps,
            generation_time=time.time() - e2e_start_time,
            agent_logs=self._agent_logs,
            client_responses=client_responses,
            response=Template(self.chat_template).render(tool_logs=self._agent_logs) if self.chat_template else None,
        )
